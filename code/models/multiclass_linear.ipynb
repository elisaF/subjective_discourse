{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "np.random.seed(RANDOM_SEED)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 5.39MB/s]                    \n",
      "2020-07-24 21:09:13 INFO: Downloading default packages for language: en (English)...\n",
      "2020-07-24 21:09:14 INFO: File exists: /Users/elisa/stanza_resources/en/default.zip.\n",
      "2020-07-24 21:09:19 INFO: Finished downloading models and saved to /Users/elisa/stanza_resources.\n",
      "2020-07-24 21:09:19 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2020-07-24 21:09:19 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-07-24 21:09:19 INFO: Use device: cpu\n",
      "2020-07-24 21:09:19 INFO: Loading: tokenize\n",
      "2020-07-24 21:09:19 INFO: Loading: pos\n",
      "2020-07-24 21:09:20 INFO: Loading: lemma\n",
      "2020-07-24 21:09:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "parser = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "def tokenize_text(text, lemmatize=False):\n",
    "    doc = parser(text)\n",
    "    if lemmatize:\n",
    "        return [word.text for sent in doc.sentences for word in sent.words]\n",
    "    else:\n",
    "        return [word.lemma for sent in doc.sentences for word in sent.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"answer\", \"answer_overans-sway\", \"shift-dodge\", \"shift-correct\", \"cant-answer-lying\",\n",
    "                \"cant-answer-sincere\"]\n",
    "\n",
    "def get_splits(splits_dir, train, dev, test):\n",
    "    train_x, train_y = get_split(splits_dir, train)\n",
    "    dev_x, dev_y = None, None\n",
    "    if dev:\n",
    "        dev_x, dev_y = get_split(splits_dir, dev)\n",
    "    test_x, test_y = get_split(splits_dir, test)\n",
    "    return train_x, train_y, dev_x, dev_y, test_x, test_y\n",
    "\n",
    "def get_split(splits_dir, split_file):\n",
    "    split_file_path = os.path.join(splits_dir, split_file)\n",
    "    with open(split_file_path) as f:\n",
    "        split_reader = csv.reader(f, delimiter='\\t')\n",
    "        split_data = list(split_reader)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for (i, line) in enumerate(split_data):\n",
    "        if i==0:\n",
    "            continue\n",
    "        label_set = line[1]\n",
    "        text = line[2]\n",
    "        y.append([int(digit) for digit in label_set])\n",
    "        x.append(text)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def search_estimator(train_x, train_y, dev_x, dev_y, estimator, parameters):\n",
    "    # perform grid search over train and dev\n",
    "    x, y, cv_train_dev = create_train_dev_cv(train_x, train_y, dev_x, dev_y)\n",
    "    \n",
    "    # define pipeline and parameters\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', estimator)\n",
    "    ])\n",
    "\n",
    "    #model = make_pipeline(PolynomialFeatures(3), estimator)\n",
    "    #model.fit(this_X, this_y)\n",
    "    #mse = mean_squared_error(model.predict(X_test), y_test)\n",
    "    run_grid_search(x, y, pipeline, parameters, cv_train_dev)\n",
    "    \n",
    "def create_train_dev_cv(train_x, train_y, dev_x, dev_y):\n",
    "    x = np.concatenate([train_x, dev_x])\n",
    "    y = np.concatenate([train_y, dev_y])\n",
    "\n",
    "    # create cv iterator object\n",
    "    test_fold = np.concatenate([\n",
    "                                    # The training data\n",
    "                                    np.ones(train_x.shape[0], dtype=np.int8)*-1,\n",
    "                                    # The development data\n",
    "                                    np.zeros(dev_x.shape[0], dtype=np.int8)])\n",
    "    cv_train_dev = PredefinedSplit(test_fold)\n",
    "\n",
    "    return x, y, cv_train_dev\n",
    "\n",
    "def run_grid_search(x, y, pipeline, parameters, cv_iter):\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=cv_iter, n_jobs=-1, verbose=1, scoring='f1_macro')\n",
    "    grid_search.fit(x, y)\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_dir = '../../data/gold/gold_cv_dev_data/CongressionalHearing/'\n",
    "train='train.tsv'\n",
    "dev=None\n",
    "test='dev.tsv'\n",
    "\n",
    "train_x, train_y, dev_x, dev_y, test_x, test_y = get_splits(splits_dir, train, dev, test)\n",
    "estimator = OneVsRestClassifier(SVC(random_state=RANDOM_SEED))\n",
    "svm_params = {\n",
    "        'vect__max_df': (0.75, 1.0),\n",
    "        'vect__min_df': (0.75, 1.0),\n",
    "        'vect__ngram_range': ((1, 1), (1, 2), (1,3)),  # unigrams, bigrams, or trigrams\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', 'l2'),\n",
    "        #'clf__estimator__kernel': ('linear','poly', 'rbf', 'sigmoid'),\n",
    "    }\n",
    "#best_parameters = search_estimator(train_x, train_y, dev_x, dev_y, estimator, svm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(clf, train_x, train_y, test_x, test_y, best_parameters):\n",
    "    clf_pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', clf),\n",
    "    ])\n",
    "    clf_pipeline.set_params(**best_parameters)\n",
    "    print(clf_pipeline)\n",
    "    clf_pipeline.fit(train_x, train_y)\n",
    "    predict_y = clf_pipeline.predict(test_x)\n",
    "    \n",
    "    \n",
    "    if isinstance(clf_pipeline['clf'], MultiOutputRegressor):\n",
    "        set_accuracy = np.sum(np.equal(test_y, predict_y).all(axis=1)) / test_y.shape[0]\n",
    "    else:\n",
    "        set_accuracy = clf_pipeline.score(test_x, test_y)\n",
    "    metrics_dict = classification_report(test_y, predict_y, target_names=class_labels, output_dict=True)\n",
    "        \n",
    "#     if isinstance(classifier, MLARAM):\n",
    "#             classifier.reset()\n",
    "    return set_accuracy, metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'tfidf__norm': 'l1', 'tfidf__use_idf': True, \n",
    "               'vect__max_df': 1.0, 'vect__min_df': 0.75,\n",
    "               'vect__ngram_range': (1, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_clf = OneVsRestClassifier(SVC(random_state=RANDOM_SEED))\n",
    "set_accuracy_svm, metrics_dict_svm = eval_classifier(svm_clf, train_x, train_y, dev_x, dev_y, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(set_accuracy_svm)\n",
    "print(metrics_dict_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_majority(train_x, train_y, test_x, test_y, dummy_strategy, constant_value=None):\n",
    "    if dummy_strategy == 'constant':\n",
    "        dummy_clf = DummyClassifier(strategy=dummy_strategy, constant=constant_value, random_state=RANDOM_SEED)\n",
    "    else:\n",
    "        dummy_clf = DummyClassifier(strategy=dummy_strategy,random_state=RANDOM_SEED)\n",
    "    dummy_clf.fit(train_x, train_y)\n",
    "    predict_y_maj = dummy_clf.predict(test_x)\n",
    "    metrics_dict = classification_report(test_y, predict_y_maj, target_names=class_labels, output_dict=True)\n",
    "    set_accuracy = dummy_clf.score(test_x, test_y)\n",
    "    return set_accuracy, metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisa/anaconda/envs/mturk/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "set_accuracy_prior, metrics_dict_prior = eval_majority(train_x, train_y, test_x, test_y, 'prior')\n",
    "set_accuracy_freq, metrics_dict_freq = eval_majority(train_x, train_y, test_x, test_y, 'most_frequent')\n",
    "set_accuracy_1, metrics_dict_1 = eval_majority(train_x, train_y, test_x, test_y, 'constant',np.ones(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior\n",
      "0.48\n",
      "{'answer': {'precision': 0.81, 'recall': 1.0, 'f1-score': 0.8950276243093923, 'support': 162}, 'answer_overans-sway': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 23}, 'shift-dodge': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 62}, 'shift-correct': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 37}, 'cant-answer-lying': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5}, 'cant-answer-sincere': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18}, 'micro avg': {'precision': 0.81, 'recall': 0.5276872964169381, 'f1-score': 0.6390532544378699, 'support': 307}, 'macro avg': {'precision': 0.135, 'recall': 0.16666666666666666, 'f1-score': 0.14917127071823205, 'support': 307}, 'weighted avg': {'precision': 0.4274267100977199, 'recall': 0.5276872964169381, 'f1-score': 0.4722947072902982, 'support': 307}, 'samples avg': {'precision': 0.81, 'recall': 0.6366666666666666, 'f1-score': 0.6916666666666665, 'support': 307}}\n",
      "Freq\n",
      "0.48\n",
      "{'answer': {'precision': 0.81, 'recall': 1.0, 'f1-score': 0.8950276243093923, 'support': 162}, 'answer_overans-sway': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 23}, 'shift-dodge': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 62}, 'shift-correct': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 37}, 'cant-answer-lying': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 5}, 'cant-answer-sincere': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 18}, 'micro avg': {'precision': 0.81, 'recall': 0.5276872964169381, 'f1-score': 0.6390532544378699, 'support': 307}, 'macro avg': {'precision': 0.135, 'recall': 0.16666666666666666, 'f1-score': 0.14917127071823205, 'support': 307}, 'weighted avg': {'precision': 0.4274267100977199, 'recall': 0.5276872964169381, 'f1-score': 0.4722947072902982, 'support': 307}, 'samples avg': {'precision': 0.81, 'recall': 0.6366666666666666, 'f1-score': 0.6916666666666665, 'support': 307}}\n",
      "Predict 1\n",
      "0.0\n",
      "{'answer': {'precision': 0.81, 'recall': 1.0, 'f1-score': 0.8950276243093923, 'support': 162}, 'answer_overans-sway': {'precision': 0.115, 'recall': 1.0, 'f1-score': 0.2062780269058296, 'support': 23}, 'shift-dodge': {'precision': 0.31, 'recall': 1.0, 'f1-score': 0.47328244274809156, 'support': 62}, 'shift-correct': {'precision': 0.185, 'recall': 1.0, 'f1-score': 0.3122362869198312, 'support': 37}, 'cant-answer-lying': {'precision': 0.025, 'recall': 1.0, 'f1-score': 0.04878048780487806, 'support': 5}, 'cant-answer-sincere': {'precision': 0.09, 'recall': 1.0, 'f1-score': 0.16513761467889906, 'support': 18}, 'micro avg': {'precision': 0.25583333333333336, 'recall': 1.0, 'f1-score': 0.40743198407431985, 'support': 307}, 'macro avg': {'precision': 0.25583333333333336, 'recall': 1.0, 'f1-score': 0.3501237472278203, 'support': 307}, 'weighted avg': {'precision': 0.526628664495114, 'recall': 1.0, 'f1-score': 0.6314381215850673, 'support': 307}, 'samples avg': {'precision': 0.2558333333333333, 'recall': 1.0, 'f1-score': 0.3970238095238095, 'support': 307}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Prior\")\n",
    "print(set_accuracy_prior)\n",
    "print(metrics_dict_prior)\n",
    "print(\"Freq\")\n",
    "print(set_accuracy_freq)\n",
    "print(metrics_dict_freq)\n",
    "print(\"Predict 1\")\n",
    "print(set_accuracy_1)\n",
    "print(metrics_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 48 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 39.8min finished\n",
      "/Users/elisa/anaconda/envs/mturk/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/elisa/anaconda/envs/mturk/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/elisa/anaconda/envs/mturk/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/elisa/anaconda/envs/mturk/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/elisa/anaconda/envs/mturk/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/elisa/anaconda/envs/mturk/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.399\n",
      "Best parameters set:\n",
      "\tclf__estimator__penalty: 'l1'\n",
      "\tclf__estimator__solver: 'saga'\n",
      "\ttfidf__norm: 'l2'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "estimator = MultiOutputRegressor(LogisticRegression(random_state=RANDOM_SEED, multi_class='multinomial'))\n",
    "# search over lr params\n",
    "lr_params = {\n",
    "        #'vect__max_df': (0.75, 1.0),\n",
    "        #'vect__min_df': (0.75, 1.0),\n",
    "        'vect__ngram_range': ((1, 2), (1,3)),  # bigrams, or trigrams\n",
    "        #'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', 'l2'),\n",
    "        'clf__estimator__penalty': ('l1', 'l2', 'elasticnet'),\n",
    "        #'clf__estimator__tol': (1e-4)\n",
    "        #'clf__estimator__C': (1.0, 0.9),\n",
    "        'clf__estimator__solver': ('newton-cg', 'sag', 'saga', 'lbfgs')\n",
    "    }\n",
    "best_parameters = search_estimator(train_x, train_y, test_x, test_y, estimator, lr_params)\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c449fc524ffa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiOutputRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mset_accuracy_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_dict_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_params' is not defined"
     ]
    }
   ],
   "source": [
    "lr_clf = MultiOutputRegressor(LogisticRegression(random_state=RANDOM_SEED, multi_class='multinomial'))\n",
    "set_accuracy_lr, metrics_dict_lr = eval_classifier(lr_clf, train_x, train_y, dev_x, dev_y, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set_accuracy_lr)\n",
    "print(metrics_dict_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.sklearn_api.ftmodel import FTTransformer\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class FTTransformer2(FTTransformer):\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        super().fit([simple_preprocess(doc) for doc in x])\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "classifiers = [OneVsRestClassifier(SVC(random_state=RANDOM_SEED))]\n",
    "#[MultiOutputRegressor(LogisticRegression(random_state=RANDOM_SEED, multi_class='multinomial'))]\n",
    "\n",
    "for classifier in classifiers:\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "            ('ftt', FTTransformer2(size=12, min_count=1, seed=0,batch_words=100)),\n",
    "            ('clf', classifier)\n",
    "        ])\n",
    "\n",
    "    print(pipeline)\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predict_y = pipeline.predict(dev_x)\n",
    "    print(classification_report(dev_y, predict_y, target_names=class_labels, output_dict=True))\n",
    "    #print(np.sum(np.equal(dev_y, predict_y).all(axis=1)) / dev_y.shape[0])\n",
    "    print(pipeline.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "\n",
    "label_orders = list(itertools.permutations(range(len(class_labels))))\n",
    "for label_order in label_orders:\n",
    "    print(\"Results for label order:\", label_order)\n",
    "    classifier = ClassifierChain(\n",
    "        classifier = SVC(),\n",
    "        require_dense = [False, True],\n",
    "        order=label_order\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', classifier)\n",
    "            ])\n",
    "\n",
    "    best_params = {'tfidf__norm': 'l1', 'tfidf__use_idf': True, \n",
    "                   'vect__max_df': 1.0, 'vect__min_df': 0.75,\n",
    "                   'vect__ngram_range': (1, 2)}\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predict_y = pipeline.predict(dev_x)\n",
    "    print(classification_report(dev_y, predict_y, target_names=class_labels, output_dict=True))\n",
    "        #print(np.sum(np.equal(dev_y, predict_y).all(axis=1)) / dev_y.shape[0])\n",
    "    print(pipeline.score(dev_x, dev_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "classifier = LabelPowerset(\n",
    "    classifier = SVC(),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', classifier)\n",
    "        ])\n",
    "\n",
    "best_params = {'tfidf__norm': 'l1', 'tfidf__use_idf': True, \n",
    "               'vect__max_df': 1.0, 'vect__min_df': 0.75,\n",
    "               'vect__ngram_range': (1, 2)}\n",
    "\n",
    "pipeline.set_params(**best_params)\n",
    "\n",
    "pipeline.fit(train_x, train_y)\n",
    "predict_y = pipeline.predict(dev_x)\n",
    "print(classification_report(dev_y, predict_y, target_names=class_labels, output_dict=True))\n",
    "#print(np.sum(np.equal(dev_y, predict_y).all(axis=1)) / dev_y.shape[0])\n",
    "print(pipeline.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.ensemble import RakelD\n",
    "n_labels = len(class_labels)\n",
    "for i in range(1,n_labels):\n",
    "    classifier = RakelD(\n",
    "        base_classifier=SVC(),\n",
    "        base_classifier_require_dense=[True, True],\n",
    "        labelset_size=i\n",
    "    )\n",
    "    pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', classifier)\n",
    "        ])\n",
    "\n",
    "    best_params = {'tfidf__norm': 'l1', 'tfidf__use_idf': True, \n",
    "                   'vect__max_df': 1.0, 'vect__min_df': 0.75,\n",
    "                   'vect__ngram_range': (1, 2)}\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predict_y = pipeline.predict(dev_x)\n",
    "    print(\"Label set size: \", i)\n",
    "    print(classification_report(dev_y, predict_y, target_names=class_labels, output_dict=True))\n",
    "        #print(np.sum(np.equal(dev_y, predict_y).all(axis=1)) / dev_y.shape[0])\n",
    "    print(pipeline.score(dev_x, dev_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.adapt import BRkNNaClassifier, BRkNNbClassifier, MLkNN, MLARAM, MLTSVM\n",
    "\n",
    "print('BRkNNaClassifier')\n",
    "for k_val in range(3,8):\n",
    "    print('k=',k_val)\n",
    "    classifier = BRkNNaClassifier(k=k_val)\n",
    "    pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', classifier)\n",
    "        ])\n",
    "\n",
    "    best_params = {'tfidf__norm': 'l1', 'tfidf__use_idf': True, \n",
    "                   'vect__max_df': 1.0, 'vect__min_df': 0.75,\n",
    "                   'vect__ngram_range': (1, 2)}\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predict_y = pipeline.predict(dev_x)\n",
    "    print(classification_report(dev_y, predict_y, target_names=class_labels, output_dict=True))\n",
    "    print(np.sum(np.equal(dev_y, predict_y).all(axis=1)) / dev_y.shape[0])\n",
    "    #print(pipeline.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BRkNNbClassifier')\n",
    "for k_val in range(4,8):\n",
    "    print('k=',k_val)\n",
    "    classifier = BRkNNbClassifier(k=k_val)\n",
    "    pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', classifier)\n",
    "        ])\n",
    "\n",
    "    best_params = {'tfidf__norm': 'l1', 'tfidf__use_idf': True, \n",
    "                   'vect__max_df': 1.0, 'vect__min_df': 0.75,\n",
    "                   'vect__ngram_range': (1, 2)}\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predict_y = pipeline.predict(dev_x)\n",
    "    print(classification_report(dev_y, predict_y, target_names=class_labels, output_dict=True))\n",
    "        #print(np.sum(np.equal(dev_y, predict_y).all(axis=1)) / dev_y.shape[0])\n",
    "    print(pipeline.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MLkNN')\n",
    "for k_val in range(1,8):\n",
    "    print('k=',k_val)\n",
    "    classifier = MLkNN(k=k_val)\n",
    "    pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', classifier)\n",
    "        ])\n",
    "\n",
    "    best_params = {'tfidf__norm': 'l1', 'tfidf__use_idf': True, \n",
    "                   'vect__max_df': 1.0, 'vect__min_df': 0.75,\n",
    "                   'vect__ngram_range': (1, 2)}\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predict_y = pipeline.predict(dev_x)\n",
    "    print(classification_report(dev_y, predict_y, target_names=class_labels, output_dict=True))\n",
    "        #print(np.sum(np.equal(dev_y, predict_y).all(axis=1)) / dev_y.shape[0])\n",
    "    print(pipeline.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MLARAM')\n",
    "for vig in [.8,.85,.9,.99]:\n",
    "    print('vigilance=',vig)\n",
    "    classifier = MLARAM(vigilance=vig)\n",
    "    pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', classifier)\n",
    "        ])\n",
    "\n",
    "    best_params = {'tfidf__norm': 'l1', 'tfidf__use_idf': True, \n",
    "                   'vect__max_df': 1.0, 'vect__min_df': 0.75,\n",
    "                   'vect__ngram_range': (1, 2)}\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predict_y = pipeline.predict(dev_x)\n",
    "    print(classification_report(dev_y, predict_y, target_names=class_labels, output_dict=True))\n",
    "        #print(np.sum(np.equal(dev_y, predict_y).all(axis=1)) / dev_y.shape[0])\n",
    "    print(pipeline.score(dev_x, dev_y))\n",
    "    classifier.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MLTSVM')\n",
    "for c_k_val in [2**i for i in range(-5, 5, 2)]:\n",
    "    print('c_k=',c_k_val)\n",
    "    classifier = MLTSVM(c_k=c_k_val)\n",
    "    pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', classifier)\n",
    "        ])\n",
    "\n",
    "    best_params = {'tfidf__norm': 'l1', 'tfidf__use_idf': True, \n",
    "                   'vect__max_df': 1.0, 'vect__min_df': 0.75,\n",
    "                   'vect__ngram_range': (1, 2)}\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predict_y = pipeline.predict(dev_x)\n",
    "    print(classification_report(dev_y, predict_y, target_names=class_labels, output_dict=True))\n",
    "    print(np.sum(np.equal(dev_y, predict_y).all(axis=1)) / dev_y.shape[0])\n",
    "    #print(pipeline.score(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{:.1f}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(ax, models, classes, data):\n",
    "    '''\n",
    "    Create a barchart for data across different categories with\n",
    "    multiple conditions for each category.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # the space between each set of bars\n",
    "    space = 0.3\n",
    "    n = len(models)\n",
    "    width = (1 - space) / (len(models))\n",
    "    \n",
    "    # Create a set of bars at each position\n",
    "    for i,model in enumerate(models):\n",
    "        indeces = range(1, len(classes)+1)\n",
    "        vals = data[i,:]\n",
    "        pos = [j - (1 - space) / 2. + i * width for j in indeces]\n",
    "        ax.bar(pos, vals, width=width, label=model)\n",
    "    \n",
    "    # Set the x-axis tick labels to be equal to the categories\n",
    "    ax.set_xticks(indeces)\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.set_ylim([0,1.2])\n",
    "    plt.setp(plt.xticks()[1], rotation=90)\n",
    "    \n",
    "    # Add the axis labels\n",
    "    ax.set_ylabel(\"F1\")\n",
    "    ax.set_xlabel(\"Response Label\")\n",
    "    \n",
    "    # Add a legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, loc='upper right')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [np.mean(majority_metrics, axis=0)[2,:], \n",
    "                 np.mean(bow1_metrics, axis=0)[2,:],\n",
    "                 np.mean(bow2_metrics, axis=0)[2,:],\n",
    "                 np.mean(bow3_metrics, axis=0)[2,:],\n",
    "                 np.mean(bow2_tfidf_norml2_metrics, axis=0)[2,:],\n",
    "                 np.mean(bow2_tfidf_norml1_metrics, axis=0)[2,:]]\n",
    "\n",
    "for svc_metric,_ in svc_metrics:\n",
    "    data.append(np.mean(svc_metric, axis=0)[2,:])\n",
    "\n",
    "data = np.array(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "barplot(ax, ['Majority', 'SVC-L-BOW-1', 'SVC-L-BOW-2', 'SVC-L-BOW-3','SVC-L-BOW1-TFIDF-L2', 'SVC-L-BOW1-TFIDF-L1', 'SVC-P-BOW-1', 'SVC-R-BOW-1', 'SVC-S-BOW-1'], class_labels, data) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
