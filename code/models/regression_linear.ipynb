{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RANDOM_SEED = 1337\n",
    "np.random.seed(RANDOM_SEED)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (\n",
    "    LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor, Ridge)\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from math import sqrt\n",
    "\n",
    "from scipy.stats import mode, kendalltau, pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 5.88MB/s]                    \n",
      "2020-07-08 12:19:04 INFO: Downloading default packages for language: en (English)...\n",
      "2020-07-08 12:19:05 INFO: File exists: /Users/elisa/stanza_resources/en/default.zip.\n",
      "2020-07-08 12:19:11 INFO: Finished downloading models and saved to /Users/elisa/stanza_resources.\n",
      "2020-07-08 12:19:11 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2020-07-08 12:19:11 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-07-08 12:19:11 INFO: Use device: cpu\n",
      "2020-07-08 12:19:11 INFO: Loading: tokenize\n",
      "2020-07-08 12:19:11 INFO: Loading: pos\n",
      "2020-07-08 12:19:12 INFO: Loading: lemma\n",
      "2020-07-08 12:19:12 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "parser = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "def tokenize_text(text, lemmatize=False):\n",
    "    doc = parser(text)\n",
    "    if lemmatize:\n",
    "        return [word.text for sent in doc.sentences for word in sent.words]\n",
    "    else:\n",
    "        return [word.lemma for sent in doc.sentences for word in sent.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(y_true, y_pred):\n",
    "    loss = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(y_pred)\n",
    "    print('rmse: ', loss)\n",
    "    return loss\n",
    "\n",
    "rmse_score = make_scorer(rmse_loss, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(splits_dir, train, dev, is_logistic=False, id_column=0, target_column=26, text_column=2):\n",
    "    train_x, train_y = get_split(splits_dir, train, is_logistic, id_column, target_column, text_column)\n",
    "    dev_x, dev_y = get_split(splits_dir, dev, is_logistic, id_column, target_column, text_column)\n",
    "    return train_x, train_y, dev_x, dev_y\n",
    "\n",
    "def get_split(splits_dir, split_file, is_logistic, id_column, target_column, text_column):\n",
    "    split_file_path = os.path.join(splits_dir, split_file)\n",
    "    with open(split_file_path) as f:\n",
    "        split_reader = csv.reader(f, delimiter='\\t')\n",
    "        split_data = list(split_reader)[1:]  # skip header\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for line in split_data:\n",
    "        if is_logistic:\n",
    "            y.append(int(float((line[target_column]))*10))\n",
    "        else:\n",
    "            y.append(float(line[target_column]))\n",
    "        x.append(line[text_column])\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dev_cv(train_x, train_y, dev_x, dev_y):\n",
    "    x = np.concatenate([train_x, dev_x])\n",
    "    y = np.concatenate([train_y, dev_y])\n",
    "\n",
    "    # create cv iterator object\n",
    "    test_fold = np.concatenate([\n",
    "                                    # The training data\n",
    "                                    np.ones(train_x.shape[0], dtype=np.int8)*-1,\n",
    "                                    # The development data\n",
    "                                    np.zeros(dev_x.shape[0], dtype=np.int8)])\n",
    "    cv_train_dev = PredefinedSplit(test_fold)\n",
    "\n",
    "    return x, y, cv_train_dev\n",
    "\n",
    "def run_grid_search(x, y, pipeline, parameters, cv_iter):\n",
    "    grid_search = GridSearchCV(pipeline, parameters, cv=cv_iter, n_jobs=-1, verbose=1, scoring='neg_root_mean_squared_error')\n",
    "    grid_search.fit(x, y)\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return best_parameters\n",
    "\n",
    "def search_linear(train_x, train_y, dev_x, dev_y):\n",
    "    best_params = []\n",
    "    # perform grid search over train and dev\n",
    "    x, y, cv_train_dev = create_train_dev_cv(train_x, train_y, dev_x, dev_y)\n",
    "    \n",
    "    estimators = [#('OLS', LinearRegression()),\n",
    "                  ('Ridge', Ridge(random_state=RANDOM_SEED)),\n",
    "                  #('KernelRidge', KernelRidge(alpha=1, kernel='linear', gamma=None, degree=3, coef0=1))\n",
    "              #('Theil-Sen', TheilSenRegressor(random_state=42)), gets error: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n",
    "              #('RANSAC', RANSACRegressor(min_samples=len(dev_x), random_state=42)),\n",
    "              #('HuberRegressor', HuberRegressor())\n",
    "                ]\n",
    "    for estimator in estimators:\n",
    "        pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        estimator,\n",
    "        ])\n",
    "        # uncommenting more parameters will give better exploring power but will\n",
    "        # increase processing time in a combinatorial way\n",
    "        parameters = {\n",
    "            #'vect__max_df': (0.75, 1.0),\n",
    "            #'vect__min_df': (0.75, 1.0),\n",
    "            'vect__ngram_range': ((1, 1), (1, 2), (1,3)),  # unigrams, bigrams, or trigrams\n",
    "            #'tfidf__use_idf': (True, False),\n",
    "            'tfidf__norm': ('l1', 'l2'),\n",
    "            'Ridge__alpha': (0.5, 1.0, 1.5),\n",
    "            'Ridge__tol': (0.0001, 0.01),\n",
    "            #'clf__estimator__kernel': ('linear','poly', 'rbf', 'sigmoid'),\n",
    "        }\n",
    "        print('Running grid search for estimator: ', estimator)\n",
    "        best_params.append(run_grid_search(x, y, pipeline, parameters, cv_train_dev))\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running grid search for estimator:  ('Ridge', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=1337, solver='auto', tol=0.001))\n",
      "Fitting 1 folds for each of 36 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 40.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.306\n",
      "Best parameters set:\n",
      "\tRidge__alpha: 1.5\n",
      "\tRidge__tol: 0.0001\n",
      "\ttfidf__norm: 'l2'\n",
      "\tvect__ngram_range: (1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'memory': None,\n",
       "  'steps': [('vect',\n",
       "    CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                    dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                    lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                    ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "                    strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                    tokenizer=<function tokenize_text at 0x7fa65e7ec7a0>,\n",
       "                    vocabulary=None)),\n",
       "   ('tfidf',\n",
       "    TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "   ('Ridge',\n",
       "    Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "          normalize=False, random_state=1337, solver='auto', tol=0.0001))],\n",
       "  'verbose': False,\n",
       "  'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                  dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                  lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                  ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "                  strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                  tokenizer=<function tokenize_text at 0x7fa65e7ec7a0>,\n",
       "                  vocabulary=None),\n",
       "  'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       "  'Ridge': Ridge(alpha=1.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "        normalize=False, random_state=1337, solver='auto', tol=0.0001),\n",
       "  'vect__analyzer': 'word',\n",
       "  'vect__binary': False,\n",
       "  'vect__decode_error': 'strict',\n",
       "  'vect__dtype': numpy.int64,\n",
       "  'vect__encoding': 'utf-8',\n",
       "  'vect__input': 'content',\n",
       "  'vect__lowercase': True,\n",
       "  'vect__max_df': 1.0,\n",
       "  'vect__max_features': None,\n",
       "  'vect__min_df': 1,\n",
       "  'vect__ngram_range': (1, 3),\n",
       "  'vect__preprocessor': None,\n",
       "  'vect__stop_words': None,\n",
       "  'vect__strip_accents': None,\n",
       "  'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "  'vect__tokenizer': <function __main__.tokenize_text(text, lemmatize=False)>,\n",
       "  'vect__vocabulary': None,\n",
       "  'tfidf__norm': 'l2',\n",
       "  'tfidf__smooth_idf': True,\n",
       "  'tfidf__sublinear_tf': False,\n",
       "  'tfidf__use_idf': True,\n",
       "  'Ridge__alpha': 1.5,\n",
       "  'Ridge__copy_X': True,\n",
       "  'Ridge__fit_intercept': True,\n",
       "  'Ridge__max_iter': None,\n",
       "  'Ridge__normalize': False,\n",
       "  'Ridge__random_state': 1337,\n",
       "  'Ridge__solver': 'auto',\n",
       "  'Ridge__tol': 0.0001}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_dir = '../../private_data/splits_folds_ordered_response_06-19/fold0'\n",
    "train='train.tsv'\n",
    "dev='test.tsv'\n",
    "\n",
    "train_x, train_y, dev_x, dev_y = get_splits(splits_dir, train, dev)\n",
    "best_params = search_linear(train_x, train_y, dev_x, dev_y)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge RMSE:  0.30574382199995925\n",
      "Ridge Kendall:  0.2030462612784842\n",
      "Ridge Pearson:  0.3683692257369324\n",
      "Ridge Spearman:  0.28202613821840333\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True, \n",
    "                                     ngram_range=(1, 3))),\n",
    "            ('tfidf', TfidfTransformer(norm='l2')),\n",
    "            ('Ridge', Ridge(alpha=1.5, tol=0.0001, random_state=RANDOM_SEED)),\n",
    "            ])\n",
    "pipeline.fit(train_x, train_y)\n",
    "preds = pipeline.predict(dev_x)\n",
    "rmse = np.sqrt(mean_squared_error(dev_y, preds))\n",
    "print('Ridge RMSE: ', rmse)\n",
    "print('Ridge Kendall: ', kendalltau(dev_y, preds)[0])\n",
    "print('Ridge Pearson: ', pearsonr(dev_y, preds)[0])\n",
    "print('Ridge Spearman: ', spearmanr(dev_y, preds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Freq entropy: 0.0\n",
      "RMSE using most freq entropy: 0.5102494215256714\n",
      "Mean entropy:  0.33980704680727497\n",
      "RMSE using mean entropy: 0.33243672623925663\n"
     ]
    }
   ],
   "source": [
    "entropy_most_freq = mode(train_y)[0][0]\n",
    "entropy_mean = np.mean(train_y)\n",
    "\n",
    "rmse_most_freq = np.sqrt(mean_squared_error(dev_y, [entropy_most_freq]*len(dev_y)))\n",
    "rmse_mean_ent = np.sqrt(mean_squared_error(dev_y, [entropy_mean]*len(dev_y)))\n",
    "print('Most Freq entropy:', entropy_most_freq)\n",
    "print('RMSE using most freq entropy:', rmse_most_freq)\n",
    "print('Mean entropy: ', entropy_mean)\n",
    "print('RMSE using mean entropy:', rmse_mean_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6598195083636408"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_y[np.where(train_y !=0)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mord'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-648366dc89a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmord\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mord'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, FunctionTransformer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import mord as m\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_estimators_logistic(train_x, train_y, dev_x, dev_y, \n",
    "                   lad_epsilon=0.0, lad_tol=0.0001, lad_C=1.0, lad_loss='l1',\n",
    "                   it_alpha=1.0,\n",
    "                   at_alpha=1.0):\n",
    "    model_to_preds = {}\n",
    "    estimators = [('LAD', m.LAD(epsilon=lad_epsilon, tol=lad_tol, C=lad_C, loss=lad_loss, fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, max_iter=1000, random_state=RANDOM_SEED)),\n",
    "                  #('LogisticIT', m.LogisticIT(alpha=it_alpha, verbose=0)),\n",
    "                  #('LogisticAT', m.LogisticAT(alpha=at_alpha, verbose=0))\n",
    "                  #Values in y must be [0 1 2 3 4]\n",
    "                 ]\n",
    "    for estimator in estimators:\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            estimator,\n",
    "            ])\n",
    "        pipeline.fit(train_x, train_y)\n",
    "        preds = pipeline.predict(dev_x)\n",
    "        rmse = np.sqrt(mean_squared_error(dev_y, preds))\n",
    "        model_to_preds[pipeline] = preds\n",
    "        print('Estimator: ', estimator, ' RMSE: ', rmse)\n",
    "    return model_to_preds\n",
    "\n",
    "def run_estimators(train_x, train_y, dev_x, dev_y, \n",
    "                   ridge_alpha=1.0, ridge_tol=0.001):\n",
    "    model_to_preds = {}\n",
    "    estimators = [('OrdinalRidge', m.OrdinalRidge(alpha=ridge_alpha, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=ridge_tol, solver='auto', random_state=RANDOM_SEED))]\n",
    "    for estimator in estimators:\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize_text, lowercase=True)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            estimator,\n",
    "            ])\n",
    "        pipeline.fit(train_x, train_y)\n",
    "        preds = pipeline.predict(dev_x)\n",
    "        rmse = np.sqrt(mean_squared_error(dev_y, preds))\n",
    "        kend = kendalltau(dev_y, preds)[0]\n",
    "        pear = pearsonr(dev_y, preds)[0]\n",
    "        spear = spearmanr(dev_y, preds)[0]\n",
    "        model_to_preds[pipeline] = preds\n",
    "        print('Estimator: ', estimator, '\\n  RMSE:', rmse, '\\tKendall:', kend, '\\tPearson:', pear, '\\tSpearman: ', spear)\n",
    "    return model_to_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splits_dir = '../../data/splits_folds_ordered_response_06-19/fold0'\n",
    "train='train.tsv'\n",
    "dev='test.tsv'\n",
    "\n",
    "train_x, train_y, dev_x, dev_y = get_splits(splits_dir, train, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ridge_alphas = [0.5, 1.0, 1.5]\n",
    "ridge_tols = [0.0001, 0.001, 0.01]\n",
    "model_dicts=[]\n",
    "for alpha in ridge_alphas:\n",
    "    for tol in ridge_tols:\n",
    "        print('alpha: ', alpha, ', tol: ', tol)\n",
    "        model_dicts.append(run_estimators(train_x, train_y, dev_x, dev_y, ridge_alpha=alpha, ridge_tol=tol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x_int, train_y_int, dev_x_int, dev_y_int = get_splits(splits_dir, train, dev, is_logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_dicts = run_estimators_logistic(train_x_int, train_y_int, dev_x_int, dev_y_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_log = np.sqrt(mean_squared_error(dev_y_int, [0]*len(dev_y_int)))\n",
    "print(rmse_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
